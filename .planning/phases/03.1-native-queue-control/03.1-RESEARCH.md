# Phase 3.1: Native Queue Control - Research

**Researched:** 2026-02-01
**Domain:** ComfyUI Queue Control - Native API Without External Node Dependencies
**Confidence:** MEDIUM

## Summary

This phase removes the Impact Pack dependency from queue control by replacing `impact-add-queue` and `impact-stop-auto-queue` events with native ComfyUI mechanisms. Research reveals that the current implementation relies on Impact Pack's frontend JavaScript handlers to listen for these events and trigger queue additions. The native approach requires either: (1) direct HTTP POST to `/prompt` endpoint to re-queue the current workflow, or (2) using `PromptServer.instance.send_sync()` with a custom message type combined with custom JavaScript to handle the event.

The key challenge is that Impact Pack's events (`impact-add-queue`, `impact-stop-auto-queue`) are NOT native ComfyUI message types - they are custom events that only work when Impact Pack's JavaScript is loaded. Without Impact Pack, these events are silently ignored by the frontend.

**Primary recommendation:** Use direct HTTP POST to `http://127.0.0.1:{port}/prompt` from within the node to re-queue the current workflow. Access the current prompt via hidden inputs (`PROMPT`, `EXTRA_PNGINFO`), and use synchronous `requests` library (not aiohttp) to avoid asyncio event loop conflicts. For stopping iteration, simply don't queue the next execution when batch completes - no special "stop" signal is needed.

## Standard Stack

The established libraries/tools for this domain:

### Core (ComfyUI-Provided)
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| `server.PromptServer` | ComfyUI built-in | Server singleton, provides address/port | Singleton pattern at `PromptServer.instance` |
| `requests` | Python stdlib-like | Synchronous HTTP POST | Avoids asyncio conflicts within node execution |
| Hidden Inputs | ComfyUI built-in | Access current workflow/prompt | `PROMPT` and `EXTRA_PNGINFO` provide full workflow data |

### Supporting (Built-in Python)
| Library | Purpose | When to Use |
|---------|---------|-------------|
| `json` | JSON serialization | Preparing POST body for /prompt |
| `uuid` | Generate unique IDs | Client ID for queue requests |
| `typing` | Type hints | Method signatures |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| HTTP POST to /prompt | `PromptServer.instance.prompt_queue.put()` | Direct queue access is possible but requires understanding internal queue format; HTTP is cleaner and documented |
| `requests` library | `urllib.request` | Both work synchronously; requests has better ergonomics |
| Hidden inputs for prompt | `prompt_queue.currently_running` | Hidden inputs are the documented approach; currently_running requires implementation knowledge |

**Installation:** No additional dependencies - all required components are ComfyUI built-ins or Python stdlib.

## Architecture Patterns

### Recommended Project Structure
```
comfyui_batch_image_processing/
├── nodes/
│   └── batch_loader.py      # Uses native queue triggering
├── utils/
│   └── queue_control.py     # MODIFIED: Native implementation
└── tests/
    └── test_queue_control.py # Updated tests
```

### Pattern 1: Native Queue Trigger via HTTP POST
**What:** POST the current workflow to `/prompt` endpoint to trigger next queue execution
**When to use:** When batch should continue to next iteration
**Example:**
```python
# Source: ComfyUI Routes Documentation + PromptServer source
import json
import requests
import uuid

try:
    from server import PromptServer
    HAS_SERVER = True
except ImportError:
    PromptServer = None
    HAS_SERVER = False

def trigger_next_queue_native(prompt: dict, extra_data: dict = None) -> bool:
    """Trigger next queue execution using native ComfyUI API.

    Args:
        prompt: The workflow prompt dict (from hidden PROMPT input)
        extra_data: Optional extra data to include

    Returns:
        True if queue was triggered, False otherwise
    """
    if not HAS_SERVER or PromptServer.instance is None:
        return False

    # Get server address and port
    address = getattr(PromptServer.instance, 'address', '127.0.0.1')
    port = getattr(PromptServer.instance, 'port', 8188)

    # Build request payload
    payload = {
        "prompt": prompt,
        "client_id": str(uuid.uuid4()),
    }
    if extra_data:
        payload["extra_data"] = extra_data

    try:
        # Use synchronous requests to avoid asyncio conflicts
        response = requests.post(
            f"http://{address}:{port}/prompt",
            json=payload,
            timeout=5
        )
        return response.status_code == 200
    except Exception:
        return False
```

### Pattern 2: Accessing Current Workflow via Hidden Inputs
**What:** Use hidden inputs to get the current workflow prompt and metadata
**When to use:** To obtain the prompt data needed for re-queueing
**Example:**
```python
# Source: ComfyUI Hidden Inputs Documentation
# https://docs.comfy.org/custom-nodes/backend/more_on_inputs

class BatchImageLoader:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "directory": ("STRING", {...}),
                # ... other required inputs
            },
            "optional": {...},
            "hidden": {
                "prompt": "PROMPT",           # Complete workflow
                "extra_pnginfo": "EXTRA_PNGINFO",  # PNG metadata
                "unique_id": "UNIQUE_ID",     # This node's ID
            }
        }

    def load_image(self, directory, ..., prompt=None, extra_pnginfo=None, unique_id=None):
        # prompt contains the complete workflow dict
        # Can be re-submitted to /prompt for next iteration
        pass
```

### Pattern 3: Batch Completion Without External Events
**What:** Stop iteration by simply not triggering the next queue
**When to use:** When batch is complete (no special "stop" signal needed)
**Example:**
```python
# Source: Logic - no Impact Pack dependency needed
def stop_auto_queue_native() -> bool:
    """Signal batch completion.

    With native queue control, stopping is implicit:
    - We simply don't POST to /prompt when batch is complete
    - No special event needed

    Returns:
        True (always succeeds - stopping means doing nothing)
    """
    # Native approach: batch completion = don't queue next
    # No event needed - the absence of a queue trigger IS the stop
    return True
```

### Pattern 4: Avoiding Asyncio Event Loop Conflicts
**What:** Use synchronous HTTP libraries instead of async within nodes
**When to use:** Always, when making HTTP requests from node execution
**Example:**
```python
# Source: GitHub Issue #9007 - asyncio.run() cannot be called from running event loop

# WRONG - causes RuntimeError
import aiohttp
import asyncio
async def bad_approach():
    async with aiohttp.ClientSession() as session:
        await session.post(...)
asyncio.run(bad_approach())  # ERROR: event loop already running

# CORRECT - use synchronous requests
import requests
def good_approach():
    response = requests.post(...)  # Works fine
    return response
```

### Anti-Patterns to Avoid
- **Using Impact Pack events (`impact-add-queue`):** These only work when Impact Pack's JS is loaded
- **Using `asyncio.run()` within nodes:** ComfyUI already has an event loop running
- **Using `aiohttp` directly in nodes:** Same asyncio conflict
- **Hardcoding port 8188:** Use `PromptServer.instance.port` for flexibility
- **Relying on external "stop" signals:** Simply don't queue next execution

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Getting server address | Parse command-line args | `PromptServer.instance.address` / `.port` | Server stores this after startup |
| Accessing current workflow | Try to reverse-engineer from state | Hidden `PROMPT` input | Documented ComfyUI feature |
| Making HTTP requests | Use aiohttp within node | Use `requests` library | Avoids asyncio event loop conflicts |
| Generating unique client IDs | Custom ID generation | `uuid.uuid4()` | Standard library, guaranteed unique |

**Key insight:** The native approach is simpler than it appears - just POST to `/prompt` with the current workflow. The complexity of Impact Pack's event system is unnecessary for this use case.

## Common Pitfalls

### Pitfall 1: Asyncio Event Loop Conflict
**What goes wrong:** `RuntimeError: asyncio.run() cannot be called from a running event loop`
**Why it happens:** ComfyUI server runs on aiohttp with an active asyncio event loop
**How to avoid:** Use synchronous `requests` library, not `asyncio.run()` or `aiohttp`
**Warning signs:** RuntimeError mentioning "running event loop"

### Pitfall 2: Impact Pack Events Silently Ignored
**What goes wrong:** `send_sync("impact-add-queue", {})` does nothing without Impact Pack
**Why it happens:** Impact Pack registers frontend JS listener for these custom events
**How to avoid:** Use native HTTP POST to `/prompt` instead
**Warning signs:** Queue trigger appears to succeed but nothing happens

### Pitfall 3: Missing Hidden Inputs
**What goes wrong:** `prompt` argument is None, cannot re-queue workflow
**Why it happens:** Hidden inputs not declared in `INPUT_TYPES`
**How to avoid:** Add `"hidden": {"prompt": "PROMPT", ...}` to INPUT_TYPES
**Warning signs:** TypeError when accessing prompt data, or prompt is None

### Pitfall 4: Hardcoded Server Address
**What goes wrong:** Queue trigger fails when ComfyUI runs on non-default port
**Why it happens:** Hardcoded `http://127.0.0.1:8188/prompt`
**How to avoid:** Use `PromptServer.instance.address` and `.port`
**Warning signs:** Connection refused errors on custom port setups

### Pitfall 5: Race Condition with Output Persistence
**What goes wrong:** Next iteration starts before current iteration's outputs are saved
**Why it happens:** Queue trigger happens during node execution, before outputs persist
**How to avoid:** This is generally safe for batch processing where each iteration is independent; outputs persist after node returns
**Warning signs:** Missing outputs or incomplete processing

### Pitfall 6: Requests Library Not Available
**What goes wrong:** `ImportError: No module named 'requests'`
**Why it happens:** Requests is not guaranteed to be in ComfyUI environment
**How to avoid:** Use `urllib.request` as fallback, or verify requests availability
**Warning signs:** Import errors on fresh ComfyUI installations

## Code Examples

Verified patterns from official sources:

### Complete Native Queue Control Implementation
```python
# Source: Synthesized from ComfyUI documentation and server.py analysis
"""Native queue control utilities for batch image processing.

Provides queue triggering without external node dependencies.
"""

import json
import uuid

# Use urllib.request for guaranteed availability (Python stdlib)
import urllib.request
import urllib.error

try:
    from server import PromptServer
    HAS_SERVER = True
except ImportError:
    PromptServer = None
    HAS_SERVER = False


def get_server_address() -> tuple[str, int]:
    """Get the ComfyUI server address and port.

    Returns:
        Tuple of (address, port). Defaults to ('127.0.0.1', 8188).
    """
    if HAS_SERVER and PromptServer.instance is not None:
        address = getattr(PromptServer.instance, 'address', '127.0.0.1')
        port = getattr(PromptServer.instance, 'port', 8188)
        return (address, port)
    return ('127.0.0.1', 8188)


def trigger_next_queue_native(prompt: dict) -> bool:
    """Trigger ComfyUI to queue another workflow execution.

    Uses native HTTP POST to /prompt endpoint. Does not require
    Impact Pack or any external custom nodes.

    Args:
        prompt: The complete workflow prompt dict

    Returns:
        True if queue trigger was sent, False if failed
    """
    if not prompt:
        return False

    address, port = get_server_address()

    payload = {
        "prompt": prompt,
        "client_id": str(uuid.uuid4()),
    }

    try:
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            f"http://{address}:{port}/prompt",
            data=data,
            headers={'Content-Type': 'application/json'},
            method='POST'
        )
        with urllib.request.urlopen(req, timeout=5) as response:
            return response.status == 200
    except (urllib.error.URLError, urllib.error.HTTPError, Exception):
        return False


def stop_auto_queue_native() -> bool:
    """Signal ComfyUI to stop auto queue when batch completes.

    With native queue control, stopping is implicit - we simply
    don't trigger the next queue. This function exists for API
    compatibility and always returns True.

    Returns:
        True (stopping = not queueing next, which always succeeds)
    """
    # Native approach: batch complete = don't queue next
    # No external event needed
    return True
```

### Updated BatchImageLoader Hidden Inputs
```python
# Source: ComfyUI Hidden Inputs Documentation
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "directory": ("STRING", {...}),
            "filter_preset": (["All Images", "PNG Only", "JPG Only", "Custom"],),
            "iteration_mode": (["Continue", "Reset"], ...),
            "error_handling": (["Stop on error", "Skip on error"], ...),
        },
        "optional": {
            "custom_pattern": ("STRING", {...}),
            "start_index": ("INT", {...}),
        },
        "hidden": {
            "prompt": "PROMPT",              # Complete workflow for re-queueing
            "extra_pnginfo": "EXTRA_PNGINFO",  # PNG metadata
            "unique_id": "UNIQUE_ID",        # This node's ID
        },
    }

def load_image(
    self,
    directory: str,
    filter_preset: str,
    iteration_mode: str = "Continue",
    error_handling: str = "Stop on error",
    custom_pattern: str = "*.png,*.jpg,*.jpeg,*.webp",
    start_index: int = 0,
    # Hidden inputs
    prompt: dict = None,
    extra_pnginfo: dict = None,
    unique_id: str = None,
):
    # ... existing logic ...

    # Queue control using native approach
    if batch_complete:
        stop_auto_queue_native()  # Just for API compatibility
    else:
        trigger_next_queue_native(prompt)  # POST to /prompt

    # ... return outputs ...
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Impact Pack events (`impact-add-queue`) | Native HTTP POST to `/prompt` | Phase 3.1 | No external dependencies |
| Impact Pack events (`impact-stop-auto-queue`) | Simply don't queue next | Phase 3.1 | Simpler logic, no dependency |
| Hardcoded port 8188 | `PromptServer.instance.port` | Best practice | Works with custom ports |

**Deprecated/outdated:**
- `send_sync("impact-add-queue", {})`: Only works with Impact Pack installed
- `send_sync("impact-stop-auto-queue", {})`: Only works with Impact Pack installed

## Open Questions

Things that couldn't be fully resolved:

1. **Requests library availability**
   - What we know: Requests may not be in all ComfyUI environments
   - What's unclear: Whether to use requests (better ergonomics) or urllib (guaranteed)
   - Recommendation: Use `urllib.request` for guaranteed availability, avoid adding dependencies

2. **Server address edge cases**
   - What we know: `PromptServer.instance.address` and `.port` exist after server starts
   - What's unclear: Behavior when address is '0.0.0.0' (listen on all interfaces)
   - Recommendation: If address is '0.0.0.0', use '127.0.0.1' for localhost HTTP calls

3. **Queue trigger timing**
   - What we know: Node execution happens in a separate thread from the server event loop
   - What's unclear: Whether triggering queue during node execution has timing issues
   - Recommendation: Test thoroughly; the HTTP approach should be safe as it's asynchronous to node execution

4. **Backward compatibility with Impact Pack users**
   - What we know: Users who have Impact Pack will still have it work
   - What's unclear: Whether to provide a fallback to Impact Pack events
   - Recommendation: Use native-only approach; Impact Pack is optional, not required

## Sources

### Primary (HIGH confidence)
- [ComfyUI Routes Documentation](https://docs.comfy.org/development/comfyui-server/comms_routes) - POST /prompt endpoint format
- [ComfyUI Hidden Inputs Documentation](https://docs.comfy.org/custom-nodes/backend/more_on_inputs) - PROMPT, EXTRA_PNGINFO, UNIQUE_ID
- [ComfyUI server.py](https://github.com/comfyanonymous/ComfyUI/blob/master/server.py) - PromptServer.instance.address and .port
- [ComfyUI Messages Documentation](https://docs.comfy.org/development/comfyui-server/comms_messages) - Native message types (confirms impact-* are not native)

### Secondary (MEDIUM confidence)
- [GitHub Issue #5667](https://github.com/comfyanonymous/ComfyUI/issues/5667) - How to trigger Queue Prompt from within a node
- [GitHub Issue #11376](https://github.com/comfyanonymous/ComfyUI/issues/11376) - Accessing prompt data from custom node
- [GitHub Issue #9007](https://github.com/Comfy-Org/ComfyUI/issues/9007) - asyncio.run() event loop conflict
- [Impact Pack logics.py](https://github.com/ltdrdata/ComfyUI-Impact-Pack/blob/Main/modules/impact/logics.py) - ImpactQueueTrigger implementation (for reference)

### Tertiary (LOW confidence)
- WebSearch results on ComfyUI queue management patterns
- Community discussions on native queue control approaches

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - PromptServer and HTTP API verified in ComfyUI source
- Architecture: MEDIUM - HTTP POST approach is documented but not commonly used for self-queuing
- Pitfalls: HIGH - asyncio conflicts well-documented in GitHub issues

**Research date:** 2026-02-01
**Valid until:** 2026-03-01 (30 days - queue API is stable)
